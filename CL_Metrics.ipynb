{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DccffCSEm9u",
        "outputId": "7fa4c855-cc79-4c75-d6a6-e29420feafd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "_rxa-gh8E8hg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip  /content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/s200r-NoMask.zip"
      ],
      "metadata": {
        "id": "iFXFOz4rO-4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = '/content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/s200r-NoMask/bert_dis_bert_fine_tune_random'\n",
        "path = 's200r-NoMask/bert_dis_bert_fine_tune_random'"
      ],
      "metadata": {
        "id": "AyHDP977FE-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best F1 Analysis"
      ],
      "metadata": {
        "id": "Nx2oHlhzlVOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# EWC/CTR/SEQ: check best f1 location\n",
        "best_f1_location_check = []\n",
        "overfit_task = []\n",
        "kt_task = []\n",
        "for seed_idx in [101,2650,0]:\n",
        "    for rand_idx in range(7):\n",
        "        list_of_lists = []\n",
        "        with open(path+str(rand_idx)+'_seed'+str(seed_idx)+'_f1.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                inner_list = [float(elt.strip()) for elt in line.split('\\t')]\n",
        "                list_of_lists.append(inner_list)\n",
        "        f1_matrix = np.array(list_of_lists)\n",
        "        assert f1_matrix.shape == (6,6)\n",
        "        temp_best = []\n",
        "        for i in range(5): # Exclude the last task\n",
        "            location = np.argmax(f1_matrix[i:,i])\n",
        "            best_f1_location_check.append(location==0)\n",
        "            if location==0:\n",
        "                overfit_task.append(i)\n",
        "            if location!=0:\n",
        "                kt_task.append(i)\n",
        "print(np.mean(best_f1_location_check))\n",
        "print('kt tasks:',Counter(kt_task))\n",
        "print('overfit tasks:',Counter(overfit_task))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGo6BamNBcKA",
        "outputId": "9acde819-011f-4c4b-fbc0-3ae04d831c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8761904761904762\n",
            "kt tasks: Counter({3: 5, 2: 4, 0: 4})\n",
            "overfit tasks: Counter({1: 21, 4: 21, 0: 17, 2: 17, 3: 16})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "2WNXtwV-lS6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1 = []\n",
        "forgetting = []\n",
        "best_f1 = []\n",
        "worst_f1 = []\n",
        "for seed_idx in [101,2650,0]:\n",
        "    for rand_idx in range(7):\n",
        "    # for rand_idx in [0,2,10,12,13,14,15]:\n",
        "        list_of_lists = []\n",
        "        with open(path+str(rand_idx)+'_seed'+str(seed_idx)+'_f1.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                inner_list = [float(elt.strip()) for elt in line.split('\\t')]\n",
        "                list_of_lists.append(inner_list)\n",
        "        f1_matrix = np.array(list_of_lists)\n",
        "        assert f1_matrix.shape == (6,6)\n",
        "        # print(f1_matrix)\n",
        "        if 'annomi' in path and 'rrr' in path and rand_idx==0:\n",
        "            print('modified for annomi')\n",
        "            overall_f1.append(np.mean(f1_matrix[3,:]))\n",
        "            temp_forgetting = []\n",
        "            for i in range(3):\n",
        "                temp_forgetting.append(np.max(f1_matrix[i:-3,i])-f1_matrix[3,i])\n",
        "            forgetting.append(np.mean(temp_forgetting))\n",
        "            temp_best = []\n",
        "            for i in range(4):\n",
        "                temp_best.append(np.max(f1_matrix[i:,i]))\n",
        "            best_f1.append(np.mean(temp_best))\n",
        "            temp_worst = []\n",
        "            for i in range(4):\n",
        "                temp_worst.append(np.min(f1_matrix[i:-2,i]))\n",
        "            worst_f1.append(np.mean(temp_worst))\n",
        "        else:\n",
        "            overall_f1.append(np.mean(f1_matrix[5,:]))\n",
        "            temp_forgetting = []\n",
        "            for i in range(5):\n",
        "                temp_forgetting.append(np.max(f1_matrix[i:-1,i])-f1_matrix[5,i])\n",
        "            forgetting.append(np.mean(temp_forgetting))\n",
        "            temp_best = []\n",
        "            for i in range(6):\n",
        "                temp_best.append(np.max(f1_matrix[i:,i]))\n",
        "            best_f1.append(np.mean(temp_best))\n",
        "            temp_worst = []\n",
        "            for i in range(6):\n",
        "                temp_worst.append(np.min(f1_matrix[i:,i]))\n",
        "            worst_f1.append(np.mean(temp_worst))\n",
        "assert len(overall_f1)==21\n",
        "assert len(forgetting)==21\n",
        "assert len(best_f1)==21\n",
        "assert len(worst_f1)==21"
      ],
      "metadata": {
        "id": "xSNnsRQ-FioB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1 = [val*100 for val in overall_f1]\n",
        "print(np.mean(overall_f1), np.std(overall_f1))\n",
        "print(np.mean(forgetting)*100)\n",
        "print(np.mean(best_f1)*100)\n",
        "print(np.mean(worst_f1)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxrsTCw1G6w-",
        "outputId": "0fab25f9-9e40-4d9d-90a2-a7a3876a9b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69.76753968253969 14.668496599577328\n",
            "12.76142857142857\n",
            "80.77111111111114\n",
            "63.282777777777774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip  /content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/s200r-EWCFreeze-v2.1.zip"
      ],
      "metadata": {
        "id": "C10mEOruMmzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path2 = '/content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/AnnoMIStr-TaskDrop/annomi_taskdrop_random'\n",
        "path2 = 's200r-EWCFreeze-v2.1/bert_dis_bert_adapter_ewc_freeze_random'"
      ],
      "metadata": {
        "id": "wKz_KXurNdI2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1_compare = []\n",
        "forgetting_compare = []\n",
        "best_f1_compare = []\n",
        "worst_f1_compare = []\n",
        "for seed_idx in [101,2650,0]:\n",
        "    # for rand_idx in range(7):\n",
        "    # for rand_idx in [0,2,10,12,13,14,15]:\n",
        "    for rand_idx in [0,3,6]:\n",
        "        list_of_lists = []\n",
        "        with open(path2+str(rand_idx)+'_seed'+str(seed_idx)+'_f1.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                inner_list = [float(elt.strip()) for elt in line.split('\\t')]\n",
        "                list_of_lists.append(inner_list)\n",
        "        f1_matrix = np.array(list_of_lists)\n",
        "        assert f1_matrix.shape == (6,6)\n",
        "        # print(f1_matrix)\n",
        "        if 'annomi' in path2 and 'replay' in path2 and rand_idx==0:\n",
        "            print('modified for annomi')\n",
        "            overall_f1_compare.append(np.mean(f1_matrix[3,:]))\n",
        "            temp_forgetting = []\n",
        "            for i in range(3):\n",
        "                temp_forgetting.append(np.max(f1_matrix[i:-3,i])-f1_matrix[3,i])\n",
        "            forgetting_compare.append(np.mean(temp_forgetting))\n",
        "            temp_best = []\n",
        "            for i in range(4):\n",
        "                temp_best.append(np.max(f1_matrix[i:,i]))\n",
        "            best_f1_compare.append(np.mean(temp_best))\n",
        "            temp_worst = []\n",
        "            for i in range(4):\n",
        "                temp_worst.append(np.min(f1_matrix[i:-2,i]))\n",
        "            worst_f1_compare.append(np.mean(temp_worst))\n",
        "        else:\n",
        "            overall_f1_compare.append(np.mean(f1_matrix[5,:]))\n",
        "            temp_forgetting = []\n",
        "            for i in range(5):\n",
        "                temp_forgetting.append(np.max(f1_matrix[i:-1,i])-f1_matrix[5,i])\n",
        "            forgetting_compare.append(np.mean(temp_forgetting))\n",
        "            temp_best = []\n",
        "            for i in range(6):\n",
        "                temp_best.append(np.max(f1_matrix[i:,i]))\n",
        "            best_f1_compare.append(np.mean(temp_best))\n",
        "            temp_worst = []\n",
        "            for i in range(6):\n",
        "                temp_worst.append(np.min(f1_matrix[i:,i]))\n",
        "            worst_f1_compare.append(np.mean(temp_worst))\n",
        "# assert len(overall_f1_compare)==21\n",
        "# assert len(forgetting_compare)==21\n",
        "# assert len(best_f1_compare)==21\n",
        "# assert len(worst_f1_compare)==21"
      ],
      "metadata": {
        "id": "h3imUlpSNoq2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1_compare = [val*100 for val in overall_f1_compare]\n",
        "print(np.mean(overall_f1_compare), np.std(overall_f1_compare))\n",
        "print(np.mean(forgetting_compare)*100)\n",
        "print(np.mean(best_f1_compare)*100)\n",
        "print(np.mean(worst_f1_compare)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKOB_IErdB_I",
        "outputId": "a56a987d-833c-48fc-ce18-12882f8971e1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74.95074074074074 10.762483366618286\n",
            "7.100888888888888\n",
            "81.72000000000001\n",
            "61.66574074074074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Significance Test"
      ],
      "metadata": {
        "id": "sGclk8rhlZxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind, ttest_rel"
      ],
      "metadata": {
        "id": "S5w_ZXwkOY2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = ttest_rel(overall_f1,overall_f1_compare, alternative='greater')\n",
        "t = res[0]\n",
        "p = res[1]\n",
        "alpha = 0.1\n",
        "# test if A > B\n",
        "if p < alpha and t > 0:\n",
        "  # reject the null hypothesis (no effect) => A > B !!\n",
        "  print('Yes!', p)\n",
        "else:\n",
        "  # accept the null hypothesis => no effect\n",
        "  print('No!', p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iQnO3IcU80i",
        "outputId": "335ac788-4235-455d-8bde-d5c87ed38108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes! 0.03357277954113428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = ttest_ind(forgetting_compare,forgetting)\n",
        "t = res[0]\n",
        "p = res[1]\n",
        "alpha = 0.1\n",
        "# test if A > B\n",
        "if p/2 < alpha and t>0:\n",
        "  # reject the null hypothesis (no effect) => A > B !!\n",
        "  print('Yes!')\n",
        "else:\n",
        "  # accept the null hypothesis => no effect\n",
        "  print('No!', p/2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh7C-PMYOo_X",
        "outputId": "6426dadb-6dee-4f72-e329-22095ef212a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modified ParamCount - EWC Freeze"
      ],
      "metadata": {
        "id": "21YLXaaOlcep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch"
      ],
      "metadata": {
        "id": "VYQ0peqgmTwM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "class CPU_Unpickler(pickle.Unpickler):\n",
        "    def find_class(self, module, name):\n",
        "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
        "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
        "        else:\n",
        "            return super().find_class(module, name)"
      ],
      "metadata": {
        "id": "EhmYxyzWndeB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path3 = 's200r-EWCFreeze-v2/modified_paramcount_random'"
      ],
      "metadata": {
        "id": "_ICa2PIFl9nJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_counts = {}\n",
        "for seed in [0,101,2650]:\n",
        "    for rand_idx in [0,3,6]:\n",
        "        for m in [1,2,3,4,5]:\n",
        "            with open(path3+str(rand_idx)+'_seed'+str(seed)+'model_'+str(m)+'.pkl', 'rb') as handle:\n",
        "                # temp = torch.load(handle, map_location=torch.device('cpu'))\n",
        "                temp = CPU_Unpickler(handle).load()\n",
        "                check_counts['random'+str(rand_idx)+'seed'+str(seed)+'model'+str(m)] = temp\n",
        "                "
      ],
      "metadata": {
        "id": "2wndK2LslgW0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in temp.items():\n",
        "  print(k,v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bogUMt12nudl",
        "outputId": "10fc4860-7690-4bc4-fcc0-27b840e37fa2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert.encoder.layer.0.attention.output.LayerNorm.weight tensor(763)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias tensor(767)\n",
            "bert.encoder.layer.0.attention.output.adapter.fc1.weight tensor(1522908)\n",
            "bert.encoder.layer.0.attention.output.adapter.fc1.bias tensor(1993)\n",
            "bert.encoder.layer.0.attention.output.adapter.fc2.weight tensor(1496034)\n",
            "bert.encoder.layer.0.attention.output.adapter.fc2.bias tensor(753)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight tensor(766)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias tensor(764)\n",
            "bert.encoder.layer.0.output.adapter.fc1.weight tensor(1526203)\n",
            "bert.encoder.layer.0.output.adapter.fc1.bias tensor(1996)\n",
            "bert.encoder.layer.0.output.adapter.fc2.weight tensor(1513178)\n",
            "bert.encoder.layer.0.output.adapter.fc2.bias tensor(763)\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight tensor(766)\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias tensor(765)\n",
            "bert.encoder.layer.1.attention.output.adapter.fc1.weight tensor(1520995)\n",
            "bert.encoder.layer.1.attention.output.adapter.fc1.bias tensor(1992)\n",
            "bert.encoder.layer.1.attention.output.adapter.fc2.weight tensor(1488377)\n",
            "bert.encoder.layer.1.attention.output.adapter.fc2.bias tensor(758)\n",
            "bert.encoder.layer.1.output.LayerNorm.weight tensor(759)\n",
            "bert.encoder.layer.1.output.LayerNorm.bias tensor(765)\n",
            "bert.encoder.layer.1.output.adapter.fc1.weight tensor(1521902)\n",
            "bert.encoder.layer.1.output.adapter.fc1.bias tensor(1996)\n",
            "bert.encoder.layer.1.output.adapter.fc2.weight tensor(1508438)\n",
            "bert.encoder.layer.1.output.adapter.fc2.bias tensor(762)\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight tensor(762)\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias tensor(763)\n",
            "bert.encoder.layer.2.attention.output.adapter.fc1.weight tensor(1524867)\n",
            "bert.encoder.layer.2.attention.output.adapter.fc1.bias tensor(1993)\n",
            "bert.encoder.layer.2.attention.output.adapter.fc2.weight tensor(1495683)\n",
            "bert.encoder.layer.2.attention.output.adapter.fc2.bias tensor(756)\n",
            "bert.encoder.layer.2.output.LayerNorm.weight tensor(765)\n",
            "bert.encoder.layer.2.output.LayerNorm.bias tensor(763)\n",
            "bert.encoder.layer.2.output.adapter.fc1.weight tensor(1524474)\n",
            "bert.encoder.layer.2.output.adapter.fc1.bias tensor(1993)\n",
            "bert.encoder.layer.2.output.adapter.fc2.weight tensor(1506766)\n",
            "bert.encoder.layer.2.output.adapter.fc2.bias tensor(760)\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight tensor(767)\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias tensor(763)\n",
            "bert.encoder.layer.3.attention.output.adapter.fc1.weight tensor(1518278)\n",
            "bert.encoder.layer.3.attention.output.adapter.fc1.bias tensor(1979)\n",
            "bert.encoder.layer.3.attention.output.adapter.fc2.weight tensor(1488245)\n",
            "bert.encoder.layer.3.attention.output.adapter.fc2.bias tensor(752)\n",
            "bert.encoder.layer.3.output.LayerNorm.weight tensor(760)\n",
            "bert.encoder.layer.3.output.LayerNorm.bias tensor(762)\n",
            "bert.encoder.layer.3.output.adapter.fc1.weight tensor(1523468)\n",
            "bert.encoder.layer.3.output.adapter.fc1.bias tensor(1992)\n",
            "bert.encoder.layer.3.output.adapter.fc2.weight tensor(1508960)\n",
            "bert.encoder.layer.3.output.adapter.fc2.bias tensor(759)\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight tensor(762)\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias tensor(756)\n",
            "bert.encoder.layer.4.attention.output.adapter.fc1.weight tensor(1513238)\n",
            "bert.encoder.layer.4.attention.output.adapter.fc1.bias tensor(1981)\n",
            "bert.encoder.layer.4.attention.output.adapter.fc2.weight tensor(1472644)\n",
            "bert.encoder.layer.4.attention.output.adapter.fc2.bias tensor(744)\n",
            "bert.encoder.layer.4.output.LayerNorm.weight tensor(760)\n",
            "bert.encoder.layer.4.output.LayerNorm.bias tensor(751)\n",
            "bert.encoder.layer.4.output.adapter.fc1.weight tensor(1519243)\n",
            "bert.encoder.layer.4.output.adapter.fc1.bias tensor(1988)\n",
            "bert.encoder.layer.4.output.adapter.fc2.weight tensor(1486096)\n",
            "bert.encoder.layer.4.output.adapter.fc2.bias tensor(752)\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight tensor(761)\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias tensor(750)\n",
            "bert.encoder.layer.5.attention.output.adapter.fc1.weight tensor(1501908)\n",
            "bert.encoder.layer.5.attention.output.adapter.fc1.bias tensor(1964)\n",
            "bert.encoder.layer.5.attention.output.adapter.fc2.weight tensor(1461043)\n",
            "bert.encoder.layer.5.attention.output.adapter.fc2.bias tensor(741)\n",
            "bert.encoder.layer.5.output.LayerNorm.weight tensor(757)\n",
            "bert.encoder.layer.5.output.LayerNorm.bias tensor(746)\n",
            "bert.encoder.layer.5.output.adapter.fc1.weight tensor(1509868)\n",
            "bert.encoder.layer.5.output.adapter.fc1.bias tensor(1975)\n",
            "bert.encoder.layer.5.output.adapter.fc2.weight tensor(1484740)\n",
            "bert.encoder.layer.5.output.adapter.fc2.bias tensor(747)\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight tensor(757)\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias tensor(742)\n",
            "bert.encoder.layer.6.attention.output.adapter.fc1.weight tensor(1509217)\n",
            "bert.encoder.layer.6.attention.output.adapter.fc1.bias tensor(1968)\n",
            "bert.encoder.layer.6.attention.output.adapter.fc2.weight tensor(1471825)\n",
            "bert.encoder.layer.6.attention.output.adapter.fc2.bias tensor(733)\n",
            "bert.encoder.layer.6.output.LayerNorm.weight tensor(751)\n",
            "bert.encoder.layer.6.output.LayerNorm.bias tensor(732)\n",
            "bert.encoder.layer.6.output.adapter.fc1.weight tensor(1513023)\n",
            "bert.encoder.layer.6.output.adapter.fc1.bias tensor(1958)\n",
            "bert.encoder.layer.6.output.adapter.fc2.weight tensor(1482568)\n",
            "bert.encoder.layer.6.output.adapter.fc2.bias tensor(741)\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight tensor(741)\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias tensor(734)\n",
            "bert.encoder.layer.7.attention.output.adapter.fc1.weight tensor(1479202)\n",
            "bert.encoder.layer.7.attention.output.adapter.fc1.bias tensor(1938)\n",
            "bert.encoder.layer.7.attention.output.adapter.fc2.weight tensor(1458461)\n",
            "bert.encoder.layer.7.attention.output.adapter.fc2.bias tensor(741)\n",
            "bert.encoder.layer.7.output.LayerNorm.weight tensor(720)\n",
            "bert.encoder.layer.7.output.LayerNorm.bias tensor(698)\n",
            "bert.encoder.layer.7.output.adapter.fc1.weight tensor(1461044)\n",
            "bert.encoder.layer.7.output.adapter.fc1.bias tensor(1894)\n",
            "bert.encoder.layer.7.output.adapter.fc2.weight tensor(1437076)\n",
            "bert.encoder.layer.7.output.adapter.fc2.bias tensor(719)\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight tensor(684)\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias tensor(677)\n",
            "bert.encoder.layer.8.attention.output.adapter.fc1.weight tensor(1394324)\n",
            "bert.encoder.layer.8.attention.output.adapter.fc1.bias tensor(1816)\n",
            "bert.encoder.layer.8.attention.output.adapter.fc2.weight tensor(1354735)\n",
            "bert.encoder.layer.8.attention.output.adapter.fc2.bias tensor(696)\n",
            "bert.encoder.layer.8.output.LayerNorm.weight tensor(651)\n",
            "bert.encoder.layer.8.output.LayerNorm.bias tensor(612)\n",
            "bert.encoder.layer.8.output.adapter.fc1.weight tensor(1413716)\n",
            "bert.encoder.layer.8.output.adapter.fc1.bias tensor(1708)\n",
            "bert.encoder.layer.8.output.adapter.fc2.weight tensor(1366161)\n",
            "bert.encoder.layer.8.output.adapter.fc2.bias tensor(675)\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight tensor(444)\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias tensor(452)\n",
            "bert.encoder.layer.9.attention.output.adapter.fc1.weight tensor(1064532)\n",
            "bert.encoder.layer.9.attention.output.adapter.fc1.bias tensor(1518)\n",
            "bert.encoder.layer.9.attention.output.adapter.fc2.weight tensor(1142218)\n",
            "bert.encoder.layer.9.attention.output.adapter.fc2.bias tensor(593)\n",
            "bert.encoder.layer.9.output.LayerNorm.weight tensor(354)\n",
            "bert.encoder.layer.9.output.LayerNorm.bias tensor(381)\n",
            "bert.encoder.layer.9.output.adapter.fc1.weight tensor(877568)\n",
            "bert.encoder.layer.9.output.adapter.fc1.bias tensor(1201)\n",
            "bert.encoder.layer.9.output.adapter.fc2.weight tensor(1096195)\n",
            "bert.encoder.layer.9.output.adapter.fc2.bias tensor(500)\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight tensor(233)\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias tensor(261)\n",
            "bert.encoder.layer.10.attention.output.adapter.fc1.weight tensor(801304)\n",
            "bert.encoder.layer.10.attention.output.adapter.fc1.bias tensor(1216)\n",
            "bert.encoder.layer.10.attention.output.adapter.fc2.weight tensor(1066330)\n",
            "bert.encoder.layer.10.attention.output.adapter.fc2.bias tensor(502)\n",
            "bert.encoder.layer.10.output.LayerNorm.weight tensor(197)\n",
            "bert.encoder.layer.10.output.LayerNorm.bias tensor(221)\n",
            "bert.encoder.layer.10.output.adapter.fc1.weight tensor(630632)\n",
            "bert.encoder.layer.10.output.adapter.fc1.bias tensor(750)\n",
            "bert.encoder.layer.10.output.adapter.fc2.weight tensor(946929)\n",
            "bert.encoder.layer.10.output.adapter.fc2.bias tensor(414)\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight tensor(89)\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias tensor(47)\n",
            "bert.encoder.layer.11.attention.output.adapter.fc1.weight tensor(508556)\n",
            "bert.encoder.layer.11.attention.output.adapter.fc1.bias tensor(730)\n",
            "bert.encoder.layer.11.attention.output.adapter.fc2.weight tensor(902287)\n",
            "bert.encoder.layer.11.attention.output.adapter.fc2.bias tensor(381)\n",
            "bert.encoder.layer.11.output.LayerNorm.weight tensor(71)\n",
            "bert.encoder.layer.11.output.LayerNorm.bias tensor(42)\n",
            "bert.encoder.layer.11.output.adapter.fc1.weight tensor(272065)\n",
            "bert.encoder.layer.11.output.adapter.fc1.bias tensor(245)\n",
            "bert.encoder.layer.11.output.adapter.fc2.weight tensor(762396)\n",
            "bert.encoder.layer.11.output.adapter.fc2.bias tensor(271)\n"
          ]
        }
      ]
    }
  ]
}