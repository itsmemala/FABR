{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DccffCSEm9u",
        "outputId": "bf714c71-51c8-4d02-9313-a858fad20ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "_rxa-gh8E8hg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip  /content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/s200r-EWCFreeze-v2.5.zip"
      ],
      "metadata": {
        "id": "iFXFOz4rO-4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/s200rME2-EWCFreeze-v2.1/bert_dis_bert_adapter_ewc_freeze_random'\n",
        "# path = 's200r-EWCFreeze-v2.5/bert_dis_bert_adapter_ewc_freeze_random'"
      ],
      "metadata": {
        "id": "AyHDP977FE-L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best F1 Analysis"
      ],
      "metadata": {
        "id": "Nx2oHlhzlVOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# EWC/CTR/SEQ: check best f1 location\n",
        "best_f1_location_check = []\n",
        "overfit_task = []\n",
        "kt_task = []\n",
        "for seed_idx in [101,2650,0]:\n",
        "    for rand_idx in range(7):\n",
        "        list_of_lists = []\n",
        "        with open(path+str(rand_idx)+'_seed'+str(seed_idx)+'_f1.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                inner_list = [float(elt.strip()) for elt in line.split('\\t')]\n",
        "                list_of_lists.append(inner_list)\n",
        "        f1_matrix = np.array(list_of_lists)\n",
        "        assert f1_matrix.shape == (6,6)\n",
        "        temp_best = []\n",
        "        for i in range(5): # Exclude the last task\n",
        "            location = np.argmax(f1_matrix[i:,i])\n",
        "            best_f1_location_check.append(location==0)\n",
        "            if location==0:\n",
        "                overfit_task.append(i)\n",
        "            if location!=0:\n",
        "                kt_task.append(i)\n",
        "print(np.mean(best_f1_location_check))\n",
        "print('kt tasks:',Counter(kt_task))\n",
        "print('overfit tasks:',Counter(overfit_task))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGo6BamNBcKA",
        "outputId": "9acde819-011f-4c4b-fbc0-3ae04d831c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8761904761904762\n",
            "kt tasks: Counter({3: 5, 2: 4, 0: 4})\n",
            "overfit tasks: Counter({1: 21, 4: 21, 0: 17, 2: 17, 3: 16})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "2WNXtwV-lS6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1 = []\n",
        "forgetting = []\n",
        "best_f1 = []\n",
        "worst_f1 = []\n",
        "# for seed_idx in [0]:\n",
        "for seed_idx in [101,2650,0]:\n",
        "    # for rand_idx in range(7):\n",
        "    # for rand_idx in [0,2,10,12,13,14,15]:\n",
        "    for rand_idx in [0,3,6]:\n",
        "    # for rand_idx in [0,2,10]:\n",
        "    # for rand_idx in [0]:\n",
        "        list_of_lists = []\n",
        "        with open(path+str(rand_idx)+'_seed'+str(seed_idx)+'_f1.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                inner_list = [float(elt.strip()) for elt in line.split('\\t')]\n",
        "                list_of_lists.append(inner_list)\n",
        "        f1_matrix = np.array(list_of_lists)\n",
        "        assert f1_matrix.shape == (6,6)\n",
        "        # print(f1_matrix)\n",
        "        # if 'annomi' in path and 'rrr' in path and rand_idx==0:\n",
        "            # print('modified for annomi')\n",
        "            # overall_f1.append(np.mean(f1_matrix[3,:]))\n",
        "            # temp_forgetting = []\n",
        "            # for i in range(3):\n",
        "            #     temp_forgetting.append(np.max(f1_matrix[i:-3,i])-f1_matrix[3,i])\n",
        "            # forgetting.append(np.mean(temp_forgetting))\n",
        "            # temp_best = []\n",
        "            # for i in range(4):\n",
        "            #     temp_best.append(np.max(f1_matrix[i:,i]))\n",
        "            # best_f1.append(np.mean(temp_best))\n",
        "            # temp_worst = []\n",
        "            # for i in range(4):\n",
        "            #     temp_worst.append(np.min(f1_matrix[i:-2,i]))\n",
        "            # worst_f1.append(np.mean(temp_worst))\n",
        "        # else:\n",
        "        overall_f1.append(np.mean(f1_matrix[5,:]))\n",
        "        temp_forgetting = []\n",
        "        for i in range(5):\n",
        "            temp_forgetting.append(np.max(f1_matrix[i:-1,i])-f1_matrix[5,i])\n",
        "        forgetting.append(np.mean(temp_forgetting))\n",
        "        temp_best = []\n",
        "        for i in range(6):\n",
        "            temp_best.append(np.max(f1_matrix[i:,i]))\n",
        "        best_f1.append(np.mean(temp_best))\n",
        "        temp_worst = []\n",
        "        for i in range(6):\n",
        "            temp_worst.append(np.min(f1_matrix[i:,i]))\n",
        "        worst_f1.append(np.mean(temp_worst))\n",
        "# assert len(overall_f1)==21\n",
        "# assert len(forgetting)==21\n",
        "# assert len(best_f1)==21\n",
        "# assert len(worst_f1)==21\n",
        "print(len(overall_f1))"
      ],
      "metadata": {
        "id": "xSNnsRQ-FioB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37922d47-9396-4926-eaee-9abf4a32db54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1 = [val*100 for val in overall_f1]\n",
        "print(np.mean(overall_f1), np.std(overall_f1))\n",
        "print(np.mean(forgetting)*100)\n",
        "print(np.mean(best_f1)*100)\n",
        "print(np.mean(worst_f1)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxrsTCw1G6w-",
        "outputId": "0c100793-82c9-4aff-e462-35d4516ee353"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71.49185185185186 10.554290551229894\n",
            "13.112222222222226\n",
            "82.83666666666666\n",
            "62.869074074074085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip  /content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/s200r-EWCFreeze-v2.1.zip"
      ],
      "metadata": {
        "id": "C10mEOruMmzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path2 = '/content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/AnnoMIStr-TaskDrop/annomi_taskdrop_random'\n",
        "path2 = 's200r-EWCFreeze-v2.1/bert_dis_bert_adapter_ewc_freeze_random'"
      ],
      "metadata": {
        "id": "wKz_KXurNdI2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1_compare = []\n",
        "forgetting_compare = []\n",
        "best_f1_compare = []\n",
        "worst_f1_compare = []\n",
        "for seed_idx in [0]:\n",
        "# for seed_idx in [101,2650,0]:\n",
        "    # for rand_idx in range(7):\n",
        "    # for rand_idx in [0,2,10,12,13,14,15]:\n",
        "    for rand_idx in [0,3,6]:\n",
        "        list_of_lists = []\n",
        "        with open(path2+str(rand_idx)+'_seed'+str(seed_idx)+'_f1.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                inner_list = [float(elt.strip()) for elt in line.split('\\t')]\n",
        "                list_of_lists.append(inner_list)\n",
        "        f1_matrix = np.array(list_of_lists)\n",
        "        assert f1_matrix.shape == (6,6)\n",
        "        # print(f1_matrix)\n",
        "        # if 'annomi' in path2 and 'replay' in path2 and rand_idx==0:\n",
        "        #     print('modified for annomi')\n",
        "        #     overall_f1_compare.append(np.mean(f1_matrix[3,:]))\n",
        "        #     temp_forgetting = []\n",
        "        #     for i in range(3):\n",
        "        #         temp_forgetting.append(np.max(f1_matrix[i:-3,i])-f1_matrix[3,i])\n",
        "        #     forgetting_compare.append(np.mean(temp_forgetting))\n",
        "        #     temp_best = []\n",
        "        #     for i in range(4):\n",
        "        #         temp_best.append(np.max(f1_matrix[i:,i]))\n",
        "        #     best_f1_compare.append(np.mean(temp_best))\n",
        "        #     temp_worst = []\n",
        "        #     for i in range(4):\n",
        "        #         temp_worst.append(np.min(f1_matrix[i:-2,i]))\n",
        "        #     worst_f1_compare.append(np.mean(temp_worst))\n",
        "        # else:\n",
        "        overall_f1_compare.append(np.mean(f1_matrix[5,:]))\n",
        "        temp_forgetting = []\n",
        "        for i in range(5):\n",
        "            temp_forgetting.append(np.max(f1_matrix[i:-1,i])-f1_matrix[5,i])\n",
        "        forgetting_compare.append(np.mean(temp_forgetting))\n",
        "        temp_best = []\n",
        "        for i in range(6):\n",
        "            temp_best.append(np.max(f1_matrix[i:,i]))\n",
        "        best_f1_compare.append(np.mean(temp_best))\n",
        "        temp_worst = []\n",
        "        for i in range(6):\n",
        "            temp_worst.append(np.min(f1_matrix[i:,i]))\n",
        "        worst_f1_compare.append(np.mean(temp_worst))\n",
        "# assert len(overall_f1_compare)==21\n",
        "# assert len(forgetting_compare)==21\n",
        "# assert len(best_f1_compare)==21\n",
        "# assert len(worst_f1_compare)==21\n",
        "print(len(overall_f1_compare))"
      ],
      "metadata": {
        "id": "h3imUlpSNoq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2a32cf-f0bc-4add-dd2c-fb65d163235c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1_compare = [val*100 for val in overall_f1_compare]\n",
        "print(np.mean(overall_f1_compare), np.std(overall_f1_compare))\n",
        "print(np.mean(forgetting_compare)*100)\n",
        "print(np.mean(best_f1_compare)*100)\n",
        "print(np.mean(worst_f1_compare)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKOB_IErdB_I",
        "outputId": "7d423288-576d-401d-a6d6-c01afed243fa"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83.22333333333334 1.8766049920791208\n",
            "1.328000000000001\n",
            "84.71222222222222\n",
            "69.97888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diaz+Chaudhary Metrics"
      ],
      "metadata": {
        "id": "PQ7D0HGjeLFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mtl = [0.8558,0.8918,0.8365,0.9120,0.8918,0.8038,0.9120]"
      ],
      "metadata": {
        "id": "PIsBF9j8kAQb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1 = []\n",
        "bwt = []\n",
        "rem = []\n",
        "pbwt = []\n",
        "intra = []\n",
        "for seed_idx in [101,2650,0]:\n",
        "    mtl_idx=0\n",
        "    for rand_idx in range(7):\n",
        "    # for rand_idx in [0,2,10,12,13,14,15]:\n",
        "        list_of_lists = []\n",
        "        with open(path+str(rand_idx)+'_seed'+str(seed_idx)+'_f1.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                inner_list = [float(elt.strip()) for elt in line.split('\\t')]\n",
        "                list_of_lists.append(inner_list)\n",
        "        f1_matrix = np.array(list_of_lists)\n",
        "        assert f1_matrix.shape == (6,6)\n",
        "        # print(f1_matrix)\n",
        "        temp_acc = []\n",
        "        for i in range(5):\n",
        "            temp_acc.append(np.mean(f1_matrix[i,:i+1]))\n",
        "        overall_f1.append(np.mean(temp_acc))\n",
        "        temp_bwt = []\n",
        "        for i in [1,2,3,4,5]:\n",
        "            for j in range(0,i):\n",
        "              temp_bwt.append(f1_matrix[i,j]-f1_matrix[j,j])\n",
        "        bwt.append(np.mean(temp_bwt))\n",
        "        rem.append(1-\n",
        "                   np.absolute(\n",
        "                       np.minimum(np.mean(temp_bwt),0)\n",
        "                       )\n",
        "                   )\n",
        "        pbwt.append(np.maximum(np.mean(temp_bwt),0))\n",
        "        intra.append(mtl[mtl_idx]-f1_matrix[5,5])\n",
        "        mtl_idx+=1\n",
        "assert len(overall_f1)==21\n",
        "assert len(bwt)==21\n",
        "assert len(rem)==21\n",
        "assert len(pbwt)==21\n",
        "assert len(intra)==21\n",
        "# print(len(overall_f1))"
      ],
      "metadata": {
        "id": "uDlPIUVkeKL8"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_f1 = [val*100 for val in overall_f1]\n",
        "print(np.mean(overall_f1), np.std(overall_f1))\n",
        "print(np.mean(bwt)*100)\n",
        "print(np.mean(rem)*100)\n",
        "print(np.mean(pbwt)*100)\n",
        "print(np.mean(intra)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FEnJnTJf21i",
        "outputId": "7a743d01-6a0d-4ebf-d9bc-4fd90c4e37e0"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53.21000793650793 3.7999063899345003\n",
            "-37.82615873015874\n",
            "62.17384126984127\n",
            "0.0\n",
            "10.55952380952381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Significance Test"
      ],
      "metadata": {
        "id": "sGclk8rhlZxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind, ttest_rel"
      ],
      "metadata": {
        "id": "S5w_ZXwkOY2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = ttest_rel(overall_f1,overall_f1_compare, alternative='greater')\n",
        "t = res[0]\n",
        "p = res[1]\n",
        "alpha = 0.1\n",
        "# test if A > B\n",
        "if p < alpha and t > 0:\n",
        "  # reject the null hypothesis (no effect) => A > B !!\n",
        "  print('Yes!', p)\n",
        "else:\n",
        "  # accept the null hypothesis => no effect\n",
        "  print('No!', p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iQnO3IcU80i",
        "outputId": "335ac788-4235-455d-8bde-d5c87ed38108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes! 0.03357277954113428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = ttest_ind(forgetting_compare,forgetting)\n",
        "t = res[0]\n",
        "p = res[1]\n",
        "alpha = 0.1\n",
        "# test if A > B\n",
        "if p/2 < alpha and t>0:\n",
        "  # reject the null hypothesis (no effect) => A > B !!\n",
        "  print('Yes!')\n",
        "else:\n",
        "  # accept the null hypothesis => no effect\n",
        "  print('No!', p/2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh7C-PMYOo_X",
        "outputId": "6426dadb-6dee-4f72-e329-22095ef212a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modified ParamCount - EWC Freeze"
      ],
      "metadata": {
        "id": "21YLXaaOlcep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch"
      ],
      "metadata": {
        "id": "VYQ0peqgmTwM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "class CPU_Unpickler(pickle.Unpickler):\n",
        "    def find_class(self, module, name):\n",
        "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
        "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
        "        else:\n",
        "            return super().find_class(module, name)"
      ],
      "metadata": {
        "id": "EhmYxyzWndeB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path3 = 's200r-EWCFreeze-v2.4/modified_paramcount_random'"
      ],
      "metadata": {
        "id": "_ICa2PIFl9nJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_counts = {}\n",
        "for seed in [0,101,2650]:\n",
        "    for rand_idx in [0,3,6]:\n",
        "        for m in [1,2,3,4,5]:\n",
        "            with open(path3+str(rand_idx)+'_seed'+str(seed)+'model_'+str(m)+'.pkl', 'rb') as handle:\n",
        "                # temp = torch.load(handle, map_location=torch.device('cpu'))\n",
        "                temp = CPU_Unpickler(handle).load()\n",
        "                check_counts['random'+str(rand_idx)+'seed'+str(seed)+'model'+str(m)] = temp\n",
        "                "
      ],
      "metadata": {
        "id": "2wndK2LslgW0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in check_counts['random6seed2650model5'].items():\n",
        "  print(k,v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bogUMt12nudl",
        "outputId": "581d1a78-2ae1-43c9-a84f-c8cc08c526a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert.encoder.layer.0.attention.output.LayerNorm.weight tensor(767)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias tensor(763)\n",
            "bert.encoder.layer.0.attention.output.adapter.fc1.weight tensor(1526263)\n",
            "bert.encoder.layer.0.attention.output.adapter.fc1.bias tensor(1993)\n",
            "bert.encoder.layer.0.attention.output.adapter.fc2.weight tensor(1514594)\n",
            "bert.encoder.layer.0.attention.output.adapter.fc2.bias tensor(760)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight tensor(767)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias tensor(760)\n",
            "bert.encoder.layer.0.output.adapter.fc1.weight tensor(1529991)\n",
            "bert.encoder.layer.0.output.adapter.fc1.bias tensor(1993)\n",
            "bert.encoder.layer.0.output.adapter.fc2.weight tensor(1525499)\n",
            "bert.encoder.layer.0.output.adapter.fc2.bias tensor(761)\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight tensor(767)\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias tensor(763)\n",
            "bert.encoder.layer.1.attention.output.adapter.fc1.weight tensor(1528659)\n",
            "bert.encoder.layer.1.attention.output.adapter.fc1.bias tensor(1991)\n",
            "bert.encoder.layer.1.attention.output.adapter.fc2.weight tensor(1522899)\n",
            "bert.encoder.layer.1.attention.output.adapter.fc2.bias tensor(763)\n",
            "bert.encoder.layer.1.output.LayerNorm.weight tensor(764)\n",
            "bert.encoder.layer.1.output.LayerNorm.bias tensor(764)\n",
            "bert.encoder.layer.1.output.adapter.fc1.weight tensor(1529676)\n",
            "bert.encoder.layer.1.output.adapter.fc1.bias tensor(1990)\n",
            "bert.encoder.layer.1.output.adapter.fc2.weight tensor(1525625)\n",
            "bert.encoder.layer.1.output.adapter.fc2.bias tensor(764)\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight tensor(765)\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias tensor(765)\n",
            "bert.encoder.layer.2.attention.output.adapter.fc1.weight tensor(1527566)\n",
            "bert.encoder.layer.2.attention.output.adapter.fc1.bias tensor(1992)\n",
            "bert.encoder.layer.2.attention.output.adapter.fc2.weight tensor(1520208)\n",
            "bert.encoder.layer.2.attention.output.adapter.fc2.bias tensor(764)\n",
            "bert.encoder.layer.2.output.LayerNorm.weight tensor(766)\n",
            "bert.encoder.layer.2.output.LayerNorm.bias tensor(764)\n",
            "bert.encoder.layer.2.output.adapter.fc1.weight tensor(1528640)\n",
            "bert.encoder.layer.2.output.adapter.fc1.bias tensor(1993)\n",
            "bert.encoder.layer.2.output.adapter.fc2.weight tensor(1522066)\n",
            "bert.encoder.layer.2.output.adapter.fc2.bias tensor(765)\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight tensor(767)\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias tensor(759)\n",
            "bert.encoder.layer.3.attention.output.adapter.fc1.weight tensor(1524715)\n",
            "bert.encoder.layer.3.attention.output.adapter.fc1.bias tensor(1985)\n",
            "bert.encoder.layer.3.attention.output.adapter.fc2.weight tensor(1517412)\n",
            "bert.encoder.layer.3.attention.output.adapter.fc2.bias tensor(759)\n",
            "bert.encoder.layer.3.output.LayerNorm.weight tensor(763)\n",
            "bert.encoder.layer.3.output.LayerNorm.bias tensor(761)\n",
            "bert.encoder.layer.3.output.adapter.fc1.weight tensor(1525775)\n",
            "bert.encoder.layer.3.output.adapter.fc1.bias tensor(1990)\n",
            "bert.encoder.layer.3.output.adapter.fc2.weight tensor(1517563)\n",
            "bert.encoder.layer.3.output.adapter.fc2.bias tensor(761)\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight tensor(765)\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias tensor(757)\n",
            "bert.encoder.layer.4.attention.output.adapter.fc1.weight tensor(1520433)\n",
            "bert.encoder.layer.4.attention.output.adapter.fc1.bias tensor(1975)\n",
            "bert.encoder.layer.4.attention.output.adapter.fc2.weight tensor(1506940)\n",
            "bert.encoder.layer.4.attention.output.adapter.fc2.bias tensor(761)\n",
            "bert.encoder.layer.4.output.LayerNorm.weight tensor(761)\n",
            "bert.encoder.layer.4.output.LayerNorm.bias tensor(753)\n",
            "bert.encoder.layer.4.output.adapter.fc1.weight tensor(1523961)\n",
            "bert.encoder.layer.4.output.adapter.fc1.bias tensor(1977)\n",
            "bert.encoder.layer.4.output.adapter.fc2.weight tensor(1514059)\n",
            "bert.encoder.layer.4.output.adapter.fc2.bias tensor(753)\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight tensor(763)\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias tensor(753)\n",
            "bert.encoder.layer.5.attention.output.adapter.fc1.weight tensor(1521973)\n",
            "bert.encoder.layer.5.attention.output.adapter.fc1.bias tensor(1972)\n",
            "bert.encoder.layer.5.attention.output.adapter.fc2.weight tensor(1511072)\n",
            "bert.encoder.layer.5.attention.output.adapter.fc2.bias tensor(751)\n",
            "bert.encoder.layer.5.output.LayerNorm.weight tensor(758)\n",
            "bert.encoder.layer.5.output.LayerNorm.bias tensor(745)\n",
            "bert.encoder.layer.5.output.adapter.fc1.weight tensor(1519538)\n",
            "bert.encoder.layer.5.output.adapter.fc1.bias tensor(1959)\n",
            "bert.encoder.layer.5.output.adapter.fc2.weight tensor(1504636)\n",
            "bert.encoder.layer.5.output.adapter.fc2.bias tensor(751)\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight tensor(759)\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias tensor(738)\n",
            "bert.encoder.layer.6.attention.output.adapter.fc1.weight tensor(1513015)\n",
            "bert.encoder.layer.6.attention.output.adapter.fc1.bias tensor(1967)\n",
            "bert.encoder.layer.6.attention.output.adapter.fc2.weight tensor(1500502)\n",
            "bert.encoder.layer.6.attention.output.adapter.fc2.bias tensor(757)\n",
            "bert.encoder.layer.6.output.LayerNorm.weight tensor(751)\n",
            "bert.encoder.layer.6.output.LayerNorm.bias tensor(730)\n",
            "bert.encoder.layer.6.output.adapter.fc1.weight tensor(1515834)\n",
            "bert.encoder.layer.6.output.adapter.fc1.bias tensor(1950)\n",
            "bert.encoder.layer.6.output.adapter.fc2.weight tensor(1494158)\n",
            "bert.encoder.layer.6.output.adapter.fc2.bias tensor(747)\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight tensor(745)\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias tensor(733)\n",
            "bert.encoder.layer.7.attention.output.adapter.fc1.weight tensor(1498032)\n",
            "bert.encoder.layer.7.attention.output.adapter.fc1.bias tensor(1916)\n",
            "bert.encoder.layer.7.attention.output.adapter.fc2.weight tensor(1491691)\n",
            "bert.encoder.layer.7.attention.output.adapter.fc2.bias tensor(747)\n",
            "bert.encoder.layer.7.output.LayerNorm.weight tensor(723)\n",
            "bert.encoder.layer.7.output.LayerNorm.bias tensor(708)\n",
            "bert.encoder.layer.7.output.adapter.fc1.weight tensor(1486094)\n",
            "bert.encoder.layer.7.output.adapter.fc1.bias tensor(1876)\n",
            "bert.encoder.layer.7.output.adapter.fc2.weight tensor(1459123)\n",
            "bert.encoder.layer.7.output.adapter.fc2.bias tensor(721)\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight tensor(686)\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias tensor(662)\n",
            "bert.encoder.layer.8.attention.output.adapter.fc1.weight tensor(1437538)\n",
            "bert.encoder.layer.8.attention.output.adapter.fc1.bias tensor(1837)\n",
            "bert.encoder.layer.8.attention.output.adapter.fc2.weight tensor(1413853)\n",
            "bert.encoder.layer.8.attention.output.adapter.fc2.bias tensor(691)\n",
            "bert.encoder.layer.8.output.LayerNorm.weight tensor(660)\n",
            "bert.encoder.layer.8.output.LayerNorm.bias tensor(623)\n",
            "bert.encoder.layer.8.output.adapter.fc1.weight tensor(1397494)\n",
            "bert.encoder.layer.8.output.adapter.fc1.bias tensor(1661)\n",
            "bert.encoder.layer.8.output.adapter.fc2.weight tensor(1347725)\n",
            "bert.encoder.layer.8.output.adapter.fc2.bias tensor(653)\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight tensor(514)\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias tensor(506)\n",
            "bert.encoder.layer.9.attention.output.adapter.fc1.weight tensor(1144517)\n",
            "bert.encoder.layer.9.attention.output.adapter.fc1.bias tensor(1529)\n",
            "bert.encoder.layer.9.attention.output.adapter.fc2.weight tensor(1188540)\n",
            "bert.encoder.layer.9.attention.output.adapter.fc2.bias tensor(592)\n",
            "bert.encoder.layer.9.output.LayerNorm.weight tensor(460)\n",
            "bert.encoder.layer.9.output.LayerNorm.bias tensor(440)\n",
            "bert.encoder.layer.9.output.adapter.fc1.weight tensor(971042)\n",
            "bert.encoder.layer.9.output.adapter.fc1.bias tensor(1248)\n",
            "bert.encoder.layer.9.output.adapter.fc2.weight tensor(1116394)\n",
            "bert.encoder.layer.9.output.adapter.fc2.bias tensor(533)\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight tensor(349)\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias tensor(367)\n",
            "bert.encoder.layer.10.attention.output.adapter.fc1.weight tensor(898925)\n",
            "bert.encoder.layer.10.attention.output.adapter.fc1.bias tensor(1219)\n",
            "bert.encoder.layer.10.attention.output.adapter.fc2.weight tensor(1121550)\n",
            "bert.encoder.layer.10.attention.output.adapter.fc2.bias tensor(522)\n",
            "bert.encoder.layer.10.output.LayerNorm.weight tensor(317)\n",
            "bert.encoder.layer.10.output.LayerNorm.bias tensor(300)\n",
            "bert.encoder.layer.10.output.adapter.fc1.weight tensor(712974)\n",
            "bert.encoder.layer.10.output.adapter.fc1.bias tensor(955)\n",
            "bert.encoder.layer.10.output.adapter.fc2.weight tensor(886871)\n",
            "bert.encoder.layer.10.output.adapter.fc2.bias tensor(425)\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight tensor(175)\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias tensor(177)\n",
            "bert.encoder.layer.11.attention.output.adapter.fc1.weight tensor(584582)\n",
            "bert.encoder.layer.11.attention.output.adapter.fc1.bias tensor(749)\n",
            "bert.encoder.layer.11.attention.output.adapter.fc2.weight tensor(922154)\n",
            "bert.encoder.layer.11.attention.output.adapter.fc2.bias tensor(409)\n",
            "bert.encoder.layer.11.output.LayerNorm.weight tensor(127)\n",
            "bert.encoder.layer.11.output.LayerNorm.bias tensor(127)\n",
            "bert.encoder.layer.11.output.adapter.fc1.weight tensor(349983)\n",
            "bert.encoder.layer.11.output.adapter.fc1.bias tensor(406)\n",
            "bert.encoder.layer.11.output.adapter.fc2.weight tensor(814534)\n",
            "bert.encoder.layer.11.output.adapter.fc2.bias tensor(326)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unstable ParamCount"
      ],
      "metadata": {
        "id": "bp-044iD-kAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch"
      ],
      "metadata": {
        "id": "9JkH7zDv-nY0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "class CPU_Unpickler(pickle.Unpickler):\n",
        "    def find_class(self, module, name):\n",
        "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
        "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
        "        else:\n",
        "            return super().find_class(module, name)"
      ],
      "metadata": {
        "id": "dih7B_pf-sGV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path3 = '/content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/s200rME3-EWCFreeze-v2.1-IC/random'"
      ],
      "metadata": {
        "id": "l6koKEDD-upg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_counts = {}\n",
        "# for seed in [0,101,2650]:\n",
        "for seed in [0]:\n",
        "    # for rand_idx in [0,3,6]:\n",
        "    for rand_idx in [0]:\n",
        "        for m in [1,2,3,4,5]:\n",
        "            with open(path3+str(rand_idx)+'_seed'+str(seed)+'model_'+str(m)+'_instability_paramcount.pkl', 'rb') as handle:\n",
        "                # temp = torch.load(handle, map_location=torch.device('cpu'))\n",
        "                temp = CPU_Unpickler(handle).load()\n",
        "                check_counts['random'+str(rand_idx)+'seed'+str(seed)+'model'+str(m)] = temp"
      ],
      "metadata": {
        "id": "U-Fr8rEG_JxP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in check_counts['random0seed0model3'].items():\n",
        "  print(k,v)\n",
        "  break"
      ],
      "metadata": {
        "id": "NMyUMdo6AwSi",
        "outputId": "d75b5a28-91af-4f25-d021-dc2ab905145a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert.encoder.layer.0.attention.output.LayerNorm.weight tensor(740)\n"
          ]
        }
      ]
    }
  ]
}