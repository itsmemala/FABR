---------------------------AnnoMI MH: Grad dir------------------------------
!python  FABR//run.py --bert_model 'bert-base-uncased' --experiment annomi --approach bert_adapter_ewc_freeze --imp function --baseline ewc_freeze --backbone bert_adapter --note random0 --idrandom 0 --seed 0 --scenario til --use_cls_wgts True --train_batch_size 32 --num_train_epochs 50 --valid_loss_es 0.002 --lr_patience 5 --lamb 0.1 --fisher_combine max --use_l1 True --l1_lamb 0.000000075 --save_metadata all --my_save_path /content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/AnnoMIMHAnalysis-MAS.1Freeze-v17.2.1/

!python  FABR//run.py --bert_model 'bert-base-uncased' --experiment annomi --approach bert_adapter_ewc_freeze --imp function --baseline ewc_freeze --backbone bert_adapter --note random0 --idrandom 0 --seed 0 --scenario til --use_cls_wgts True --train_batch_size 32 --num_train_epochs 50 --valid_loss_es 0.002 --lr_patience 5 --lamb 0.1 --fisher_combine max --use_l1 True --l1_lamb 0.000000075 --my_save_path /content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/AnnoMIMHAnalysis-MAS.1Freeze-v20.1/

---------------------------AnnoMI SH: MAS with 0 LA epochs--------------------

!python  FABR//run.py --bert_model 'bert-base-uncased' --experiment annomi --approach bert_adapter_ewc_freeze --imp function --baseline ewc_freeze --backbone bert_adapter --note random0 --idrandom 0 --seed 0 --scenario dil --use_cls_wgts True --train_batch_size 32 --num_train_epochs 50 --la_num_train_epochs 0 --valid_loss_es 0.002 --lr_patience 5 --lamb 0.1 --fisher_combine max --use_l1 True --l1_lamb 0.000000075 --my_save_path /content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/AnnoMIStrSH-MAS.2Freeze-v21/
---------------------------AAAI Review Expts-----------------------------------
(Intent MH EWC) L2 Loss:
-------------------------
!python  FABR//run.py --bert_model 'bert-base-uncased' --experiment hwu64 --approach bert_adapter_ewc_freeze --baseline ewc_freeze --backbone bert_adapter --note random0 --idrandom 0 --seed 0 --scenario til --train_batch_size 32 --num_train_epochs 50 --valid_loss_es 0.02 --lr_patience 5 --learning_rate 0.003 --lamb 5000000 --fisher_combine max --use_l2 True --l2_lamb 0.000075 --my_save_path /content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/hwuMH-EWC-L2/

!python  FABR//run.py --bert_model 'bert-base-uncased' --experiment hwu64 --approach bert_adapter_ewc_freeze --baseline ewc_freeze --backbone bert_adapter --note random0 --idrandom 0 --seed 0 --scenario til --train_batch_size 32 --num_train_epochs 50 --valid_loss_es 0.02 --lr_patience 5 --learning_rate 0.003 --lamb 5000000 --fisher_combine max --use_l2 True --l2_lamb 0.0001 --my_save_path /content/gdrive/MyDrive/s200_kan_myocc_attributions_lfa/hwuMH-EWC-L2.1/